{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# A wild IsolationForest has appeared(!)\n",
    "\n",
    "This document shows the results of me playing around with a IsolationForest tutorial. IsolationForest is an algorithm presented the paper cited below, linked in the tutorial at [scikit tutorials](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html). \n",
    "\n",
    "It randomly selects a feature in the training dataset and then splits between the maximum and minimum for that feature, generating a tree. Those with shorter tree paths, on average, can be considered anomalies. \n",
    "\n",
    "I don't really have a deep understanding of what is going on here, but hopefully I will next quarter after I take machine learning! \n",
    "\n",
    ".. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\" Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n",
    "\n",
    "Potentially interesting readings (for myself !):\n",
    "\n",
    "[Intro blog post to anomaly detection](https://iwringer.wordpress.com/2015/11/17/anomaly-detection-concepts-and-techniques/)\n",
    "\n",
    "[LSTM Nueral network for anomaly detection](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2015-56.pdf)\n",
    "\n",
    "[RNN for anomaly detection](https://arxiv.org/abs/1702.00833)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from math import log\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "def printResults(yPred, cont, loss, X_train):\n",
    "\n",
    "    num = 0\n",
    "    value = 0\n",
    "\n",
    "    for i in range(len(yPred)):\n",
    "\n",
    "        if yPred[i] == -1:\n",
    "            if DEBUG:\n",
    "                print(i+1)\n",
    "        else:\n",
    "            num += 1\n",
    "            value += X_train[i,1]\n",
    "\n",
    "    print (\"Proportion of contamination: \" + str(float(cont)/100))\n",
    "    print (\"Loss: \" + str(log(sum(loss))))\n",
    "    print (\"Number of samples labeled non-anomalies: \" + str(num))\n",
    "    print (\"Final AOV based on anomaly detection: \" + str(float(value)/num) + \"\\n\")\n",
    "\n",
    "    \n",
    "def preprocess():\n",
    "    dF = None\n",
    "\n",
    "    # get data into a pandas dataframe\n",
    "    with open(\"shop.csv\", 'rb') as data:\n",
    "        dF = pd.read_csv(data)\n",
    "\n",
    "    # turn payment_type into categorical integer where cash = 0, credit = 1, debit = 2\n",
    "    X_train = dF.as_matrix(columns=['user_id','order_amount','total_items','payment_method'])\n",
    "    payment_enc = pd.factorize(X_train[:,3])\n",
    "    X_train[:,3] = payment_enc[0]\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "\n",
    "def trainAndOutput(X_train):\n",
    "    \n",
    "    # find the best value for contamination parameter\n",
    "    for cont in range(1,8,1):\n",
    "        \n",
    "        # fit the model\n",
    "        clf = IsolationForest(contamination=float(cont)/100, n_estimators=1000, max_features=4)\n",
    "        clf.fit(X_train)\n",
    "\n",
    "        yPred = clf.predict(X_train)\n",
    "        printResults(yPred, cont, clf.decision_function(X_train), X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of contamination: 0.01\n",
      "Loss: 4.838689427573604\n",
      "Number of samples labeled non-anomalies: 4950\n",
      "Final AOV based on anomaly detection: 389.72545454545457\n",
      "\n",
      "Proportion of contamination: 0.02\n",
      "Loss: 4.814770417469775\n",
      "Number of samples labeled non-anomalies: 4900\n",
      "Final AOV based on anomaly detection: 303.43795918367346\n",
      "\n",
      "Proportion of contamination: 0.03\n",
      "Loss: 4.795452841153745\n",
      "Number of samples labeled non-anomalies: 4850\n",
      "Final AOV based on anomaly detection: 293.65051546391754\n",
      "\n",
      "Proportion of contamination: 0.04\n",
      "Loss: 4.791997923126564\n",
      "Number of samples labeled non-anomalies: 4800\n",
      "Final AOV based on anomaly detection: 289.71208333333334\n",
      "\n",
      "Proportion of contamination: 0.05\n",
      "Loss: 4.851792297869462\n",
      "Number of samples labeled non-anomalies: 4750\n",
      "Final AOV based on anomaly detection: 286.14736842105265\n",
      "\n",
      "Proportion of contamination: 0.06\n",
      "Loss: 4.80017472865933\n",
      "Number of samples labeled non-anomalies: 4700\n",
      "Final AOV based on anomaly detection: 282.67936170212766\n",
      "\n",
      "Proportion of contamination: 0.07\n",
      "Loss: 4.803152440195409\n",
      "Number of samples labeled non-anomalies: 4650\n",
      "Final AOV based on anomaly detection: 280.0159139784946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    X_train = preprocess()\n",
    "    trainAndOutput(X_train)\n",
    "\n",
    "main()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, it seems that if we think there are about 70-80 outliers that need to be removed from the dataset (as per analysis in the other notebook), the AOV above is pretty close to what we calculated in our other document - the above output shows somewhere between 389-303 as AOV.\n",
    "\n",
    "Obviously the above method is random, so it would be problematic to say that the above proves anything. However, what the above method does seem to support is that our AOV calculation makes sense - as seen in the second batch of data, if we were to eliminate about 100 suspicious data points from the purchases, leaving 4900 purchases, we would get an AOV of around 303."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
